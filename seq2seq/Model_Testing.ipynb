{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b520d7-8a3b-47e3-aed6-117b2bc57ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af605a25-8dbc-4d56-beae-a47952ab397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data.csv\")\n",
    "df_filtered = df[['agnostic', 'semantic']]\n",
    "\n",
    "agn_vocab_file = \"../agnostic_vocab.txt\"\n",
    "sem_vocab_file = \"../semantic_vocab.txt\"\n",
    "\n",
    "with open(agn_vocab_file, 'r') as file:\n",
    "    agn_vocab = file.read().splitlines()\n",
    "with open(sem_vocab_file, 'r') as file:\n",
    "    sem_vocab = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9f9498-da38-4a5b-8b4a-b8841e1c1578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69619/828857444.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_pre = df_filtered.applymap(lambda x: x.split('\\t')[0:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70142\n",
      "Validation size: 8768\n",
      "Test size: 8768\n"
     ]
    }
   ],
   "source": [
    "df_pre = df_filtered.applymap(lambda x: x.split('\\t')[0:-1])\n",
    "\n",
    "train_data, temp_data = train_test_split(df_pre, test_size=0.2, random_state=42)\n",
    "\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(validation_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "def transform_row(row):\n",
    "    return {\n",
    "        'agnostic': ' '.join(row['agnostic']),  # Convert the list to a string\n",
    "        'semantic': ' '.join(row['semantic']),  # Convert the list to a string\n",
    "        'agnostic_tokens': ['<sos>'] + row['agnostic'] + ['<eos>'],  # Add <sos> and <eos>\n",
    "        'semantic_tokens': ['<sos>'] + row['semantic'] + ['<eos>']   # Add <sos> and <eos>\n",
    "    }\n",
    "\n",
    "train_data = train_data.apply(transform_row, axis=1).tolist()\n",
    "val_data = validation_data.apply(transform_row, axis=1).tolist()\n",
    "test_data = test_data.apply(transform_row, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "160611b5-b543-4464-99f6-a798bcbea197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary creation\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens_list):\n",
    "        self.special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        self.token_to_index = {tok: idx for idx, tok in enumerate(self.special_tokens)}\n",
    "        self.index_to_token = {idx: tok for tok, idx in self.token_to_index.items()}\n",
    "        self.build_vocab(tokens_list)\n",
    "\n",
    "    def build_vocab(self, tokens_list):\n",
    "        for tokens in tokens_list:\n",
    "            for token in tokens:\n",
    "                if token not in self.token_to_index:\n",
    "                    idx = len(self.token_to_index)\n",
    "                    self.token_to_index[token] = idx\n",
    "                    self.index_to_token[idx] = token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        return self.token_to_index.get(token, self.token_to_index['<unk>'])\n",
    "\n",
    "    def id_to_token(self, idx):\n",
    "        return self.index_to_token.get(idx, '<unk>')\n",
    "\n",
    "    def tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_id(token) for token in tokens]\n",
    "\n",
    "    def ids_to_tokens(self, ids):\n",
    "        return [self.id_to_token(idx) for idx in ids]\n",
    "\n",
    "agnostic_vocab = Vocabulary([d['agnostic_tokens'] for d in train_data + val_data + test_data])\n",
    "semantic_vocab = Vocabulary([d['semantic_tokens'] for d in train_data + val_data + test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b89e9de-53f4-4b37-b64b-87fbb0fc1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, agnostic_vocab, semantic_vocab):\n",
    "        self.data = data\n",
    "        self.agnostic_vocab = agnostic_vocab\n",
    "        self.semantic_vocab = semantic_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        agnostic_tokens = self.data[idx]['agnostic_tokens']\n",
    "        semantic_tokens = self.data[idx]['semantic_tokens']\n",
    "        agnostic_ids = self.agnostic_vocab.tokens_to_ids(agnostic_tokens)\n",
    "        semantic_ids = self.semantic_vocab.tokens_to_ids(semantic_tokens)\n",
    "        return torch.tensor(agnostic_ids), torch.tensor(semantic_ids)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MusicDataset(train_data, agnostic_vocab, semantic_vocab)\n",
    "validation_dataset = MusicDataset(validation_data, agnostic_vocab, semantic_vocab)\n",
    "test_dataset = MusicDataset(test_data, agnostic_vocab, semantic_vocab)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66cc37a-720d-49e1-bc28-b301097c741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs: [src_len, batch_size, hidden_dim]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "# Attention mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        \n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden state for each source token\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs.permute(1, 0, 2)), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Softmax over attention weights\n",
    "        return nn.functional.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder with attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "\n",
    "        input = input.unsqueeze(0)  # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch_size, embedding_dim]\n",
    "\n",
    "        # Attention\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)  # [batch_size, src_len]\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "\n",
    "        # Weighted sum of encoder outputs\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n",
    "        weighted = weighted.permute(1, 0, 2)  # [1, batch_size, hidden_dim]\n",
    "\n",
    "        # Combine embedded input and weighted encoder context\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [1, batch_size, embedding_dim + hidden_dim]\n",
    "\n",
    "        # Pass through RNN\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        # Final output prediction\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0)), dim=1))  # [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Update: Get encoder_outputs\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b01911d-61e2-4dc2-8e45-5cd872fe031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69619/2687959111.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./best_model_2_257_at.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "input_dim = len(agnostic_vocab)\n",
    "output_dim = len(semantic_vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "attention = Attention(hidden_dim)\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_dim, embedding_dim, hidden_dim, n_layers, dropout, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"./best_model_2_257_at.pt\"))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4f4239f-9778-4cc7-af58-ee4bbd818e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(array1, array2):\n",
    "    len1, len2 = len(array1), len(array2)\n",
    "    \n",
    "    # Initialize a 2D DP table\n",
    "    dp = [[0 for _ in range(len2 + 1)] for _ in range(len1 + 1)]\n",
    "    \n",
    "    # Base cases: distance from an empty array\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i  # Deletions\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j  # Insertions\n",
    "    \n",
    "    # Fill the DP table\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if array1[i - 1] == array2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]  # No edit needed\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],     # Deletion\n",
    "                    dp[i][j - 1],     # Insertion\n",
    "                    dp[i - 1][j - 1]  # Substitution\n",
    "                )\n",
    "    \n",
    "    return dp[len1][len2]\n",
    "\n",
    "def translate_random_test_example(model, test_data, agnostic_vocab, semantic_vocab, max_len=50):\n",
    "\n",
    "    # Randomly select a test example\n",
    "    test_example = random.choice(test_data)\n",
    "    input_tokens = test_example['agnostic_tokens']\n",
    "    expected_output_tokens = test_example['semantic_tokens']\n",
    "\n",
    "    # Translate using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Add <sos> and <eos> to the input string\n",
    "        input_tokens_with_sos_eos = [\"<sos>\"] + input_tokens + [\"<eos>\"]\n",
    "        input_ids = agnostic_vocab.tokens_to_ids(input_tokens_with_sos_eos)\n",
    "        input_tensor = torch.tensor(input_ids).unsqueeze(1).to(device)  # Add batch dimension\n",
    "\n",
    "        # Pass through the encoder\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(input_tensor)\n",
    "\n",
    "        # Initialize the decoder with <sos> token\n",
    "        trg_indexes = [semantic_vocab.token_to_id('<sos>')]\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.tensor([trg_indexes[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell, encoder_outputs)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "            if pred_token == semantic_vocab.token_to_id('<eos>'):\n",
    "                break\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "    output_tokens = semantic_vocab.ids_to_tokens(trg_indexes[1:-1])\n",
    "    input_tokens = input_tokens[1:-1]\n",
    "    expected_output_tokens = expected_output_tokens[1:-1]\n",
    "\n",
    "    \n",
    "    # Print input, expected output, and model's output\n",
    "    print(f\"Input String:\\n\\n{' '.join(input_tokens)}\\n\")\n",
    "    print(f\"Expected Output:\\n\\n{' '.join(expected_output_tokens)}\\n\")\n",
    "    print(f\"Model Output:\\n\\n{' '.join(output_tokens)}\\n\")\n",
    "    print(f\"Levenshtien Distance: {levenshtein_distance(expected_output_tokens, output_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22a1d72f-55f5-4894-90ac-ea91accd236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String:\n",
      "\n",
      "clef.C-L1 accidental.sharp-S2 accidental.sharp-L1 metersign.C-L3 rest.eighth-L3 note.eighth-S3 note.eighth-S3 note.eighth-S3 note.eighth-S3 note.eighth-S4 rest.quarter-L3 barline-L1 rest.eighth-L3 note.eighth-S3 note.eighth-S3 note.eighth-S3 note.eighth-S3 note.eighth-S4\n",
      "\n",
      "Expected Output:\n",
      "\n",
      "clef-C1 keySignature-DM timeSignature-C rest-eighth note-A4_eighth note-A4_eighth note-A4_eighth note-A4_eighth note-C#5_eighth rest-quarter barline rest-eighth note-A4_eighth note-A4_eighth note-A4_eighth note-A4_eighth note-C#5_eighth\n",
      "\n",
      "Model Output:\n",
      "\n",
      "clef-C1 keySignature-AM timeSignature-C rest-eighth note-A4_eighth note-A4_eighth note-A4_eighth note-A4_eighth note-C#5_eighth rest-quarter barline rest-eighth note-A4_eighth note-A4_eighth note-A4_eighth note-A4_eighth note-C#5_eighth\n",
      "\n",
      "Levenshtien Distance: 1\n"
     ]
    }
   ],
   "source": [
    "translate_random_test_example(model, test_data, agnostic_vocab, semantic_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d238e-7655-44e3-a6f8-67a6e79ca2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a2c0c-1981-4b0a-83f7-ad3775efe919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
