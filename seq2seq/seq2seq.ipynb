{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6468e0f4-17e0-4b78-83be-fdb3f011fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98394bbf-ab2e-46a9-87ab-b5784cf12c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data.csv\")\n",
    "df_filtered = df[['agnostic', 'semantic']]\n",
    "\n",
    "agn_vocab_file = \"../agnostic_vocab.txt\"\n",
    "sem_vocab_file = \"../semantic_vocab.txt\"\n",
    "\n",
    "with open(agn_vocab_file, 'r') as file:\n",
    "    agn_vocab = file.read().splitlines()\n",
    "with open(sem_vocab_file, 'r') as file:\n",
    "    sem_vocab = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14958881-62df-455e-b5e3-98cfb6c748cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68347/1045124681.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_pre = df_filtered.applymap(lambda x: x.split('\\t')[0:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70142\n",
      "Validation size: 8768\n",
      "Test size: 8768\n"
     ]
    }
   ],
   "source": [
    "df_pre = df_filtered.applymap(lambda x: x.split('\\t')[0:-1])\n",
    "\n",
    "train_data, temp_data = train_test_split(df_pre, test_size=0.2, random_state=42)\n",
    "\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(validation_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "def transform_row(row):\n",
    "    return {\n",
    "        'agnostic': ' '.join(row['agnostic']),  # Convert the list to a string\n",
    "        'semantic': ' '.join(row['semantic']),  # Convert the list to a string\n",
    "        'agnostic_tokens': ['<sos>'] + row['agnostic'] + ['<eos>'],  # Add <sos> and <eos>\n",
    "        'semantic_tokens': ['<sos>'] + row['semantic'] + ['<eos>']   # Add <sos> and <eos>\n",
    "    }\n",
    "\n",
    "train_data = train_data.apply(transform_row, axis=1).tolist()\n",
    "validation_data = validation_data.apply(transform_row, axis=1).tolist()\n",
    "test_data = test_data.apply(transform_row, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c470aa5b-6ee6-4994-aece-599c4bd27d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seeds for reproducibility\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a31ffc-d55b-4f7c-ba8a-eea2e2f0d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary creation\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokens_list):\n",
    "        self.special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        self.token_to_index = {tok: idx for idx, tok in enumerate(self.special_tokens)}\n",
    "        self.index_to_token = {idx: tok for tok, idx in self.token_to_index.items()}\n",
    "        self.build_vocab(tokens_list)\n",
    "\n",
    "    def build_vocab(self, tokens_list):\n",
    "        for tokens in tokens_list:\n",
    "            for token in tokens:\n",
    "                if token not in self.token_to_index:\n",
    "                    idx = len(self.token_to_index)\n",
    "                    self.token_to_index[token] = idx\n",
    "                    self.index_to_token[idx] = token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        return self.token_to_index.get(token, self.token_to_index['<unk>'])\n",
    "\n",
    "    def id_to_token(self, idx):\n",
    "        return self.index_to_token.get(idx, '<unk>')\n",
    "\n",
    "    def tokens_to_ids(self, tokens):\n",
    "        return [self.token_to_id(token) for token in tokens]\n",
    "\n",
    "    def ids_to_tokens(self, ids):\n",
    "        return [self.id_to_token(idx) for idx in ids]\n",
    "\n",
    "agnostic_vocab = Vocabulary([d['agnostic_tokens'] for d in train_data + validation_data + test_data])\n",
    "semantic_vocab = Vocabulary([d['semantic_tokens'] for d in train_data + validation_data + test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c5a9b4-4343-4915-8eb7-6e997c216055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, agnostic_vocab, semantic_vocab):\n",
    "        self.data = data\n",
    "        self.agnostic_vocab = agnostic_vocab\n",
    "        self.semantic_vocab = semantic_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        agnostic_tokens = self.data[idx]['agnostic_tokens']\n",
    "        semantic_tokens = self.data[idx]['semantic_tokens']\n",
    "        agnostic_ids = self.agnostic_vocab.tokens_to_ids(agnostic_tokens)\n",
    "        semantic_ids = self.semantic_vocab.tokens_to_ids(semantic_tokens)\n",
    "        return torch.tensor(agnostic_ids), torch.tensor(semantic_ids)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MusicDataset(train_data, agnostic_vocab, semantic_vocab)\n",
    "validation_dataset = MusicDataset(validation_data, agnostic_vocab, semantic_vocab)\n",
    "test_dataset = MusicDataset(test_data, agnostic_vocab, semantic_vocab)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: x)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f2bb30-890b-453e-b8c3-450c2cdda0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs: [src_len, batch_size, hidden_dim]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "# Attention mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "        \n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden state for each source token\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_dim]\n",
    "\n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs.permute(1, 0, 2)), dim=2)))  # [batch_size, src_len, hidden_dim]\n",
    "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
    "        \n",
    "        # Softmax over attention weights\n",
    "        return nn.functional.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder with attention\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input: [batch_size]\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        # cell: [n_layers, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_len, batch_size, hidden_dim]\n",
    "\n",
    "        input = input.unsqueeze(0)  # [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch_size, embedding_dim]\n",
    "\n",
    "        # Attention\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)  # [batch_size, src_len]\n",
    "        attention_weights = attention_weights.unsqueeze(1)  # [batch_size, 1, src_len]\n",
    "\n",
    "        # Weighted sum of encoder outputs\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hidden_dim]\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)  # [batch_size, 1, hidden_dim]\n",
    "        weighted = weighted.permute(1, 0, 2)  # [1, batch_size, hidden_dim]\n",
    "\n",
    "        # Combine embedded input and weighted encoder context\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # [1, batch_size, embedding_dim + hidden_dim]\n",
    "\n",
    "        # Pass through RNN\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        # Final output prediction\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0)), dim=1))  # [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Update: Get encoder_outputs\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429a130b-5e7b-43df-9b3e-65cb39274f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "input_dim = len(agnostic_vocab)\n",
    "output_dim = len(semantic_vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "attention = Attention(hidden_dim)\n",
    "encoder = Encoder(input_dim, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_dim, embedding_dim, hidden_dim, n_layers, dropout, attention)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=semantic_vocab.token_to_id('<pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd147dd2-e8f4-4b20-9e23-50c1a7e89c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=\"training_log.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5afb786-eb08-4d09-b8e1-76181993f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src, trg = zip(*batch)\n",
    "        src = nn.utils.rnn.pad_sequence(src, padding_value=agnostic_vocab.token_to_id('<pad>')).to(device)\n",
    "        trg = nn.utils.rnn.pad_sequence(trg, padding_value=semantic_vocab.token_to_id('<pad>')).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Batch {i+1}/{len(data_loader)}: Loss {loss.item():.4f}', end='\\r')\n",
    "    print()  # Move to the next line after the epoch\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate_fn(model, data_loader, criterion, agnostic_vocab, semantic_vocab):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_sequences = 0\n",
    "    total_symbols = 0\n",
    "    incorrect_sequences = 0\n",
    "    incorrect_symbols = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            src, trg = zip(*batch)\n",
    "            src = nn.utils.rnn.pad_sequence(src, padding_value=agnostic_vocab.token_to_id('<pad>')).to(device)\n",
    "            trg = nn.utils.rnn.pad_sequence(trg, padding_value=semantic_vocab.token_to_id('<pad>')).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing during validation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Convert predictions to token IDs\n",
    "            predicted_ids = output.argmax(dim=1).view(-1)\n",
    "            target_ids = trg.view(-1)\n",
    "            \n",
    "            # Calculate sequence-level errors\n",
    "            for pred_seq, true_seq in zip(\n",
    "                predicted_ids.split(trg.shape[0] // len(batch)),\n",
    "                target_ids.split(trg.shape[0] // len(batch))\n",
    "            ):\n",
    "                total_sequences += 1\n",
    "                total_symbols += len(true_seq)\n",
    "                if not torch.equal(pred_seq, true_seq):\n",
    "                    incorrect_sequences += 1\n",
    "                    incorrect_symbols += (pred_seq != true_seq).sum().item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    sequence_error_rate = incorrect_sequences / total_sequences\n",
    "    symbol_error_rate = incorrect_symbols / total_symbols\n",
    "\n",
    "    return avg_loss, sequence_error_rate, symbol_error_rate\n",
    "\n",
    "# Testing function\n",
    "def test_fn(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            src, trg = zip(*batch)\n",
    "            src = nn.utils.rnn.pad_sequence(src, padding_value=agnostic_vocab.token_to_id('<pad>')).to(device)\n",
    "            trg = nn.utils.rnn.pad_sequence(trg, padding_value=semantic_vocab.token_to_id('<pad>')).to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0)  # No teacher forcing during testing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            predictions.append(output.argmax(-1))  # Store predicted tokens for analysis\n",
    "    return epoch_loss / len(data_loader), predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abc6f0f-d50a-4124-8f20-9ae76f0fed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution loop\n",
    "def execute(model, train_loader, validation_loader, test_loader, optimizer, criterion, n_epochs, clip, teacher_forcing_ratio=0.5):\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        teacher_forcing_ratio = max(0.5 * (1 - epoch / n_epochs), 0.1)\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        train_loss = train_fn(model, train_loader, optimizer, criterion, clip, teacher_forcing_ratio)\n",
    "        valid_loss, seq_er, sym_er = validate_fn(model, validation_loader, criterion, agnostic_vocab, semantic_vocab)\n",
    "        print(f'Training Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f}')\n",
    "        print(f'Sequence error: {seq_er:.4f} | Symbol error {sym_er:.4f}')\n",
    "        logging.info(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "        logging.info(f\"Training Loss: {train_loss:.4f}\")\n",
    "        logging.info(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "        logging.info(f\"Sequence Error Rate: {seq_er:.4f}\")\n",
    "        logging.info(f\"Symbol Error Rate: {sym_er:.4f}\")\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')  # Save the best model\n",
    "            print('Model saved!')\n",
    "    # Load the best model and test\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    test_loss, predictions = test_fn(model, test_loader, criterion)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a00d2-b202-43e8-82df-ce187a031ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Batch 548/548: Loss 2.6463\n",
      "Training Loss: 3.6160 | Validation Loss: 2.6884\n",
      "Sequence error: 0.9919 | Symbol error 0.8230\n",
      "Model saved!\n",
      "Epoch 2/1000\n",
      "Batch 548/548: Loss 1.6535\n",
      "Training Loss: 1.9868 | Validation Loss: 1.3358\n",
      "Sequence error: 0.9807 | Symbol error 0.6290\n",
      "Model saved!\n",
      "Epoch 3/1000\n",
      "Batch 266/548: Loss 1.0356\r"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 1000\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.7\n",
    "\n",
    "# Execute the training, validation, and testing process\n",
    "predictions = execute(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    n_epochs=n_epochs,\n",
    "    clip=clip,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0178e-4a32-43a6-b34b-03e233d98c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_random_test_example(model, test_data, agnostic_vocab, semantic_vocab, max_len=50):\n",
    "    # Randomly select a test example\n",
    "    test_example = random.choice(test_data)\n",
    "    input_tokens = test_example['agnostic_tokens']\n",
    "    expected_output_tokens = test_example['semantic_tokens']\n",
    "    \n",
    "    \n",
    "    # Translate using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Add <sos> and <eos> to the input string\n",
    "        input_tokens_with_sos_eos = [\"<sos>\"] + input_tokens + [\"<eos>\"]\n",
    "        input_ids = agnostic_vocab.tokens_to_ids(input_tokens_with_sos_eos)\n",
    "        input_tensor = torch.tensor(input_ids).unsqueeze(1).to(device)  # Add batch dimension\n",
    "\n",
    "        # Pass through the encoder\n",
    "        hidden, cell = model.encoder(input_tensor)\n",
    "\n",
    "        # Initialize the decoder with <sos> token\n",
    "        trg_indexes = [semantic_vocab.token_to_id('<sos>')]\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.tensor([trg_indexes[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "            if pred_token == semantic_vocab.token_to_id('<eos>'):\n",
    "                break\n",
    "\n",
    "        # Convert token IDs to tokens\n",
    "        output_tokens = semantic_vocab.ids_to_tokens(trg_indexes[1:-1])  # Exclude <sos> and <eos>\n",
    "\n",
    "    # Print input, expected output, and model's output\n",
    "    print(f\"Input String: {input_tokens}\\n\\n\")\n",
    "    print(f\"Expected Output: {expected_output_tokens}\\n\\n\")\n",
    "    print(f\"Model Output: {output_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef470303-d220-4bef-97ed-de5105a0fd87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translate_random_test_example(model, test_data, agnostic_vocab, semantic_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b6643-eaac-417c-ba43-ff793aa950f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a3b60-d5b1-466c-9940-a83b52d90c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
